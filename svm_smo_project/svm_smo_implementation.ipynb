{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project on Support Vector Machines\n",
    "## Optimization Techniques\n",
    "### Instructor: E. Markakis\n",
    "### Team: Natasa Farmaki(DS3517018),  Stratos Gounidellis(DS3517005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines Overview\n",
    "\n",
    "Support Vector Machines is a supervised machine learning algorithm which can be used for either classification or regression tasks. Indeed, it is considered to be one of the best supervised learning algorithms.  Given a set of training examples, each of them assigned to a class (either to -1 or to 1 in the simple case) we aim to predict the class of the test examples in a classification task. Using the SVM machine learning algorithm we aim to compute that hyperplane that separates the two classes better.\n",
    "\n",
    "Support vectors are simply the closest vectors, from each class, to the classifier. Moreover, apart from linear classification Support Vector Machines can be used for non-linear classification. In order to achieve that we use the kernel trick, i.e. we implicitly map the input into high-dimensional feature spaces. Examples of kernel functions are:\n",
    "1. Radial basis functions (for appropriate values of σ)\n",
    "2. Polynomial (for appropriate values of q)\n",
    "3. Hyperbolic tangent (for appropriate values of β and γ)\n",
    "\n",
    "Sources: [https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/, http://cs229.stanford.edu/notes/cs229-notes3.pdf]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Minimal Optimization Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequential Minimal Optimization is an algorithm invented by John Platt in 1998 at Microsoft Research. It was accepted with a lot of excitement as till then training of SVMs required the use of expensive and relatively slow quadratic programming software. \n",
    "\n",
    "More specifically, training a support vector machine requires solving a very large quadratic programming optimization problem. However, according to the SMO approach that very large problem is broken into smaller subproblems that are tractable and can be solved analytically. That approach is memory efficient as there is no need to store the constraint matrix. Therefore, SMO is capable of handling large training sets while it is faster for linear SVMs and sparse data sets. Indeed, according to Platt's paper SMO can be up to 1000 times faster than the chunking algorithm on realword sparse data sets.\n",
    "\n",
    "When implementing the algorithm several decisions should be taken, such as:\n",
    "- How to pick the pair of variables that will be examined in each iteration,\n",
    "- How to pick an initial solution,\n",
    "- How to set the termination criterion.\n",
    "\n",
    "All those decisions are clarified through comments in the source code as well as in the report accompanying the project.\n",
    "\n",
    "Source: [https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import time\n",
    "from scipy.linalg import norm\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from copy import copy, deepcopy\n",
    "import pandas as pd\n",
    "from sympy import *\n",
    "import mpmath\n",
    "from scipy.spatial.distance import cdist\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the data to classify with the SVM classifier\n",
    "def read_data(file, len_lines):\n",
    "\n",
    "    # output vector\n",
    "    target = []\n",
    "    # training point matrix\n",
    "    point = []\n",
    "    temp = []\n",
    "    \n",
    "    # open the file and store the lines\n",
    "    with open(file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.read().split('\\n')\n",
    "\n",
    "    if len_lines is None or len_lines > len(lines):\n",
    "        len_lines = len(lines)\n",
    "\n",
    "    for line in lines[:len_lines]:\n",
    "\n",
    "        if (len(line)):\n",
    "            # split each line on the whitespace char\n",
    "            observation = line.strip().split(' ')\n",
    "            features = observation[1:]\n",
    "            for feature in features:\n",
    "                # store the features of each observation\n",
    "                temp.append(float(feature.strip().split(':')[1]))\n",
    "            if (len(temp)) != 4955:\n",
    "                temp = []\n",
    "                continue\n",
    "            # append the features of the current observation\n",
    "            point.append(temp)\n",
    "            # append the class of the current observation\n",
    "            target.append(float(observation[0]))\n",
    "            temp = []\n",
    "\n",
    "    target = np.asarray(target)\n",
    "    point = np.asmatrix(point)\n",
    "    # return the two matrices\n",
    "    return target, point\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Functions\n",
    "\n",
    "### SMO Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute and store the rbf(gaussian) kernel between X and Y\n",
    "def gaussianMatrix(X, Y, sigma):\n",
    "    pairwise_sq_dists = cdist(X, Y, 'sqeuclidean')\n",
    "    kernel_matrix = scipy.exp(-pairwise_sq_dists * 2 / sigma * 2)\n",
    "    return kernel_matrix\n",
    "\n",
    "\n",
    "# compute and store the linear kernel between X and Y\n",
    "def linearMatrix(X, Y):\n",
    "    linear_matrix = np.dot(X, Y.T)\n",
    "    return linear_matrix\n",
    "\n",
    "\n",
    "# return the linear(gaussian) kernel stored in the above matrices\n",
    "def kernel(i, j, kernel_matrix, k=\"linear\", sigma=1):\n",
    "    if k == \"linear\":\n",
    "        return kernel_matrix[i, j]\n",
    "    elif k == \"gaussian\":\n",
    "        return kernel_matrix[i, j]\n",
    "\n",
    "    \n",
    "# choose the second alpha to optimize according to the second choice heuristic and call\n",
    "# takeStep method.\n",
    "def examineExample(i2, point, target, C, kernel_matrix, k=\"linear\", sigma=1, tol=0.001):\n",
    "    # declare global variables\n",
    "    global alphas\n",
    "    global w\n",
    "    global beta\n",
    "    global E\n",
    "\n",
    "    # get label, lagrange multiplier and error for element in position i2\n",
    "    y2 = target[i2]\n",
    "    alpha2 = alphas[i2]\n",
    "    E2 = E[i2]\n",
    "    r2 = E2 * y2\n",
    "\n",
    "    # find positions of non-bound examples\n",
    "    non_bound = (np.where(alphas != 0) and np.where(alphas != C))\n",
    "    # if error is within tolerance\n",
    "    if ((r2 < -tol and alpha2 < C) or (r2 > tol and alpha2 > 0)):\n",
    "        # if number of non-zero and non-C alpha is greater than 1\n",
    "        # use second choice heuristic\n",
    "        if(len(non_bound[0]) > 1):\n",
    "            if E2 > 0:\n",
    "                # choose minimum error if e2 is positive\n",
    "                i1 = np.argmin(E)\n",
    "            elif(E2 < 0):\n",
    "                # choose maximum error if e2 is negative\n",
    "                i1 = np.argmax(E)\n",
    "            if takeStep(i1, i2, target, point, C, kernel_matrix, k, sigma):\n",
    "                return 1\n",
    "\n",
    "        # loop over all non-zero and non-C alpha starting at a random point\n",
    "        random_index = non_bound[0]\n",
    "        random_index = random_index[np.random.permutation(len(random_index))]\n",
    "        for i in random_index:\n",
    "            # choose identity of current alpha\n",
    "            if (alphas[i] == alphas[i2]):\n",
    "                i1 = i\n",
    "                if takeStep(i1, i2, target, point,  C, kernel_matrix, k, sigma):\n",
    "                    return 1\n",
    "        # loop over all possible indices starting at a random point\n",
    "        temp = np.arange(0, len(alphas))\n",
    "        temp = temp[np.random.permutation(len(temp))]\n",
    "        for i in temp:\n",
    "            # loop variable\n",
    "            i1 = i\n",
    "            if takeStep(i1, i2, target, point, C, kernel_matrix, k, sigma):\n",
    "                return 1\n",
    "    return 0\n",
    "\n",
    "                \n",
    "# update threshold, weight vector(if linear svm), error cache, alphas\n",
    "def takeStep(i1, i2, target, point, C, kernel_matrix, k=\"linear\", sigma=1, eps=0.001):\n",
    "    # declare global variables\n",
    "    global alphas\n",
    "    global w\n",
    "    global beta\n",
    "    global E\n",
    "\n",
    "    # if alphas are the same return 0\n",
    "    if i1 == i2:\n",
    "        return 0\n",
    "    # get label, lagrange multiplier and error for elements in position i1, i2\n",
    "    alpha1, alpha2 = alphas[i1], alphas[i2]\n",
    "    y1, y2 = target[i1], target[i2]\n",
    "    E1, E2 = E[i1], E[i2]\n",
    "\n",
    "    s = y1 * y2\n",
    "\n",
    "    # compute L, H depending on the values of the labes y1,y2\n",
    "    if y1 != y2:\n",
    "        L = max(0, alpha2 - alpha1)\n",
    "        H = min(C, C + alpha2 - alpha1)\n",
    "    else:\n",
    "        L = max(0, alpha2 + alpha1 - C)\n",
    "        H = min(C, alpha2 + alpha1)\n",
    "\n",
    "    if (L == H):\n",
    "        return 0\n",
    "\n",
    "    # calculate the second derivative of the objective function\n",
    "    # along the diagonal line, i.e. eta\n",
    "    k11 = kernel(i1, i1, kernel_matrix, k, sigma)\n",
    "    k12 = kernel(i1, i2, kernel_matrix, k, sigma)\n",
    "    k22 = kernel(i2, i2, kernel_matrix, k, sigma)\n",
    "    eta = k11 + k22 - 2 * k12\n",
    "    # if the second derivative is positive, update alpha2\n",
    "    # using the following formula\n",
    "    if (eta > 0):\n",
    "        a2 = alpha2 + y2*(E1 - E2) / eta\n",
    "        # clip a2 according to bounds\n",
    "        if (a2 < L):\n",
    "            a2 = L\n",
    "        elif (a2 > H):\n",
    "            a2 = H\n",
    "    else:\n",
    "        # evaluate the objective function at each end of the line\n",
    "        # segment\n",
    "        f1 = y1 * (E1 + beta) - alpha1 * k11 - s * alpha2 * k12\n",
    "        f2 = y2 * (E2 + beta) - s * alpha1 * k12 - alpha2 * k22\n",
    "        L1 = alpha1 + s * (alpha2 - L)\n",
    "        H1 = alpha1 + s * (alpha2 - H)\n",
    "        # objective function at a2 = L\n",
    "        obj_L = L1 * f1 + L * f2 + (1/2) * ((L1)**2) * k11 + (1/2) * (L**2) * k22 + s * L * L1 * k12\n",
    "        # objective function at a2 = H\n",
    "        obj_H = H1 * f1 + H * f2 + (1/2) * ((H1)**2) * k11 + (1/2) * (H**2) * k22 + s * H * H1 * k12\n",
    "        if (obj_L < (obj_H - eps)):\n",
    "            a2 = L\n",
    "        elif (obj_L > (obj_H + eps)):\n",
    "            a2 = H\n",
    "        else:\n",
    "            a2 = alpha2\n",
    "\n",
    "    # if a2 very close to zero or C set a to 0 or C respectively\n",
    "    if a2 < (10**(-8)):\n",
    "        a2 = 0.0\n",
    "    elif(a2 > C - (10**-8)):\n",
    "        a2 = C\n",
    "    # if difference between a_new and a_old is negligible return 0\n",
    "    if(abs(a2 - alpha2) < (eps*(a2 + alpha2 + eps))):\n",
    "        return 0\n",
    "\n",
    "    # compute new alpha1\n",
    "    a1 = alpha1 + s * (alpha2 - a2)\n",
    "\n",
    "    # compute threshold b\n",
    "    b1 = E1 + y1 * (a1 - alpha1) * k11 + y2 * (a2 - alpha2) * k12 + beta\n",
    "    b2 = E2 + y1 * (a1 - alpha1) * k12 + y2 * (a2 - alpha2) * k22 + beta\n",
    "\n",
    "    # According to Platt's SMO paper 2.3 threshold b is b1 if a1 is not in\n",
    "    #  bounds, b2 is when a2 is not at bounds and (b1+b2)/2\n",
    "    # when both a1, a2 are at bounds.\n",
    "    beta_new = 0\n",
    "    if 0 < a1 < C:\n",
    "        beta_new = b1\n",
    "    elif 0 < a2 < C:\n",
    "        beta_new = b2\n",
    "    else:\n",
    "        beta_new = (b1 + b2) / 2\n",
    "\n",
    "    t1 = y1 * (a1 - alpha1)\n",
    "    t2 = y2 * (a2 - alpha2)\n",
    "\n",
    "    # update weight vector to reflect change in alpha1 and alpha2 if SVM is linear\n",
    "    if (k == \"linear\"):\n",
    "        w = w + t1 * point[i1, :] + t2 * point[i2, :]\n",
    "\n",
    "    delta_b = beta - beta_new\n",
    "\n",
    "    # update error cache using new lagrange multipliers\n",
    "    for i in range(len(E)):\n",
    "        E[i] = E[i] + t1 * kernel(i1, i, kernel_matrix, k, sigma) + \\\n",
    "                t2 * kernel(i2, i, kernel_matrix, k, sigma) + delta_b\n",
    "\n",
    "    # set error of optimized examples to 0 if new alphas are not at bounds\n",
    "    for index in [i1, i2]:\n",
    "        if 0.0 < alphas[index] < C:\n",
    "            E[index] = 0.0\n",
    "\n",
    "    # update threshold to reflect change in lagrange multipliers\n",
    "    beta = beta_new\n",
    "    # store a1 in the alpha array\n",
    "    alphas[i1] = a1\n",
    "    # store a2 in the alpha array\n",
    "    alphas[i2] = a2\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def smo(target, point, C=1, k=\"linear\", sigma=1, tol=0.001):\n",
    "    global alphas\n",
    "    global beta\n",
    "    global w\n",
    "    global E\n",
    "\n",
    "    # initialize gaussian kernels\n",
    "    if k == \"gaussian\":\n",
    "        kernel_matrix = gaussianMatrix(point, point, sigma)\n",
    "    elif k == \"linear\":\n",
    "        kernel_matrix = linearMatrix(point, point)\n",
    "\n",
    "    # initialize w with zero\n",
    "    w = np.zeros(len(point.T))\n",
    "\n",
    "    # initialize error with minus target, as u is zero in the initial iteration\n",
    "    # accordind to Platt's equation E = u - y\n",
    "    E = -deepcopy(target)\n",
    "\n",
    "    # initialize alphas with zero to satisfy the constraint that the\n",
    "    # product of the alphas and the targe vector should equal zero.\n",
    "    alphas = np.zeros(len(point))\n",
    "\n",
    "    # initialize beta with zero\n",
    "    beta = 0.0\n",
    "\n",
    "    numChanged = 0\n",
    "    examineAll = 1\n",
    "    while (numChanged > 0 or examineAll == 1):\n",
    "        numChanged = 0\n",
    "        if (examineAll == 1):\n",
    "            # loop over all training examples\n",
    "            for i in range(len(point)):\n",
    "                numChanged += examineExample(i, point, target, C,\n",
    "                                             kernel_matrix, k, sigma, tol)\n",
    "        else:\n",
    "            # loop over all non zero and non C alphas\n",
    "            for i in (np.where(alphas != 0) and np.where(alphas != C))[0]:\n",
    "                numChanged += examineExample(i, point, target, C,\n",
    "                                             kernel_matrix, k, sigma, tol)\n",
    "        if(examineAll == 1):\n",
    "            examineAll = 0\n",
    "        elif (numChanged == 0):\n",
    "            examineAll = 1\n",
    "    # return lungrage multipliers, threshold and weights\n",
    "    return alphas, beta, w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_label(us):\n",
    "    \n",
    "    # initialize an empty predictions' list\n",
    "    pred = []\n",
    "    for u in us:\n",
    "        # if u is greater than zero\n",
    "        if u > 0:\n",
    "            # append 1 in the predictions' list\n",
    "            pred.append(1)\n",
    "        # if u is greater than zero\n",
    "        elif(u < 0):\n",
    "            # append -1 in the predictions' list\n",
    "            pred.append(-1)\n",
    "        # in the extreme case that u equals zero\n",
    "        else:\n",
    "            # append in the predictions' list\n",
    "            # either 1 or -1\n",
    "            choice = np.random.choice([-1, 1], 1)\n",
    "            pred.append(choice)\n",
    "    # return predictions\n",
    "    return pred\n",
    "\n",
    "# tune the c parameter for the linear SVM\n",
    "# for the tuning a training and a development set is used\n",
    "def tune_c(X_train, y_train, X_dev, y_dev, Cs, kernel=\"linear\"):\n",
    "    \n",
    "    # initialize metrics' lists\n",
    "    predict = []\n",
    "    acc_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_score_list = []\n",
    "    \n",
    "    print(\"--- Tuning ---\")\n",
    "    for c in Cs:\n",
    "        print(\"C =\", c)\n",
    "        # execute the smo algorithm for the current c\n",
    "        alpha, beta, w = smo(y_train, X_train, c)\n",
    "        us = eval_svm(alpha, beta, X_train, X_dev, y_train, kernel)\n",
    "        # append the accuracy and the other metrics for the current c\n",
    "        acc_list.append(accuracy(us, y_dev))\n",
    "        recall_list.append(recall_score(predict_label(us), y_dev))\n",
    "        precision_list.append(precision_score(predict_label(us), y_dev))\n",
    "        f1_score_list.append(f1_score(predict_label(us), y_dev))\n",
    "    \n",
    "    # create a dataframe including all the metrics\n",
    "    df = pd.DataFrame({\"C\": Cs, \"Accuracy\": acc_list, \"Recall\": recall_list,\n",
    "                      \"Precision\": precision_list, \"F1_score\": f1_score_list})\n",
    "    df = df[['C', 'Accuracy', 'Recall', 'Precision', 'F1_score']]\n",
    "    display(df)\n",
    "    idx = np.argmax(acc_list)\n",
    "    best_c = Cs[idx]\n",
    "    \n",
    "    # print the best accuracy achieved as well as the best C\n",
    "    print(\"Best Accuracy (\", acc_list[idx], \") is achieved for C =\", best_c)\n",
    "\n",
    "    # store in pickle best C\n",
    "    C_linear = open(\"C_linear\", \"wb\")\n",
    "    pickle.dump(best_c, C_linear)\n",
    "    C_linear.close()\n",
    "\n",
    "    # store in pickle dataframe for linear SVM\n",
    "    df_linear = open(\"df_linear\", \"wb\")\n",
    "    pickle.dump(df, df_linear)\n",
    "    df_linear.close()\n",
    "\n",
    "    return best_c, acc_list[idx], df\n",
    "\n",
    "# tune c and sigma parameters for the SVM with a Gaussian Kernel\n",
    "# for the tuning a training and a development set is used\n",
    "def tune_gaussian(X_train, y_train, X_dev, y_dev, Cs, sigmas, k=\"gaussian\"):\n",
    "    \n",
    "    # initialize metrics' lists\n",
    "    acc_list = []\n",
    "    recall_list = []\n",
    "    precision_list = []\n",
    "    f1_score_list = []\n",
    "    \n",
    "    \n",
    "    print(\"--- Tuning ---\")\n",
    "    for c in Cs:\n",
    "        for sigma in sigmas:\n",
    "            print(\"C:\", c, \"sigma:\", sigma)\n",
    "            # execute the smo algorithm for the current c and sigma\n",
    "            alpha, beta, w = smo(y_train, X_train, c, k, sigma)\n",
    "            us = eval_svm(alpha, beta, X_train, X_dev, y_train, k, sigma)\n",
    "            # append the accuracy and the other metrics for the current c and sigma\n",
    "            acc_list.append(accuracy(us, y_dev))\n",
    "            recall_list.append(recall_score(predict_label(us), y_dev))\n",
    "            precision_list.append(precision_score(predict_label(us), y_dev))\n",
    "            f1_score_list.append(f1_score(predict_label(us), y_dev))\n",
    "\n",
    "    list_c = []\n",
    "    for i in range(len(Cs)):\n",
    "        temp_c = [[Cs[i]] * len(sigmas)][0]\n",
    "        list_c += temp_c\n",
    "\n",
    "    list_sigmas = []\n",
    "    for i in range(len(np.unique(Cs))):\n",
    "        list_sigmas += sigmas\n",
    "    \n",
    "    # create a dataframe including all the metrics\n",
    "    df_gaussian = pd.DataFrame({\"C\": list_c, \"Sigma\": list_sigmas, \"Accuracy\": acc_list, \"Recall\": recall_list,\n",
    "                      \"Precision\": precision_list, \"F1_score\": f1_score_list})\n",
    "    df_gaussian = df_gaussian[['C', \"Sigma\", 'Accuracy', 'Recall', 'Precision', 'F1_score']]\n",
    "    display(df_gaussian)\n",
    "\n",
    "    idx = np.argmax(acc_list)\n",
    "    best_c = list_c[idx]\n",
    "    best_sigma = list_sigmas[idx]\n",
    "    # print the best accuracy achieved as well as the best C and sigma\n",
    "    print(\"Best Accuracy (\", acc_list[idx], \") is achieved for C =\", best_c, \" and sigma =\", best_sigma)\n",
    "\n",
    "    # store in pickle best C\n",
    "    C_gaussian = open(\"C_gaussian\", \"wb\")\n",
    "    pickle.dump(best_c, C_gaussian)\n",
    "    C_gaussian.close()\n",
    "\n",
    "    # store in pickle best sigma\n",
    "    sigma_gaussian = open(\"sigma_gaussian\", \"wb\")\n",
    "    pickle.dump(best_sigma, sigma_gaussian)\n",
    "    sigma_gaussian.close()\n",
    "\n",
    "    # store dataframe into pickle file\n",
    "    gaussiandf = open(\"df_gaussian\", \"wb\")\n",
    "    pickle.dump(df_gaussian, gaussiandf)\n",
    "    gaussiandf.close()\n",
    "\n",
    "    return best_c, best_sigma, acc_list[idx], df_gaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the results of the svm classifier\n",
    "def eval_svm(alphas, beta, train, test, target, k=\"linear\", sigma=1):\n",
    "    u = []\n",
    "    # if the svm classifier is linear\n",
    "    if k == \"linear\":\n",
    "        # compute the predictions using the weights\n",
    "        for i in range(len(test)):\n",
    "            sum_u = np.dot(w, test[i].T) - beta\n",
    "            u.append(sum_u)\n",
    "            sum_u = 0\n",
    "    # if the svm classifier uses the gaussian kernel\n",
    "    elif k == \"gaussian\":\n",
    "        # initalize the kernel matrix\n",
    "        kernel_matrix = gaussianMatrix(train, test, sigma)\n",
    "        sum_u = 0\n",
    "        non_zero = np.where(alphas != 0)[0]\n",
    "        # compute the predictions using the non-zero alphas\n",
    "        for i in range(len(test)):\n",
    "            for j in non_zero:\n",
    "                if len(non_zero):\n",
    "                    sum_u += target[j] * alphas[j] * kernel(j, i, kernel_matrix, k, sigma)\n",
    "            u.append(sum_u - beta)\n",
    "            sum_u = 0\n",
    "    return u\n",
    "\n",
    "# compute the accuracy of the svm classifier\n",
    "# based on the real target\n",
    "def accuracy(u, target):\n",
    "    labels = predict_label(u)\n",
    "    return accuracy_score(labels, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline(y_train, y_dev):\n",
    "    if len(y_train[y_train==1]) > len(y_train[y_train==-1]):\n",
    "        predictions = np.ones(len(y_dev)).ravel()\n",
    "    elif len(y_train[y_train==1]) < len(y_train[y_train==-1]):\n",
    "        predictions = -np.ones(len(y_dev)).ravel()\n",
    "    else:\n",
    "        choice = np.random.choice([-1, 1], 1)\n",
    "        if choice == -1:\n",
    "            predictions = -np.ones(len(y_dev)).ravel()\n",
    "        else:\n",
    "            predictions = np.ones(len(y_dev)).ravel()\n",
    "    baseline_acc = accuracy_score(y_dev, predictions)\n",
    "    print(\"Baseline Accuracy:\",baseline_acc)\n",
    "    return baseline_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check whether a matrix is symmetric,\n",
    "# i.e. A == A.T\n",
    "def check_symmetric(a, tol=1e-8):\n",
    "    return np.allclose(a, a.T, atol=tol)\n",
    "\n",
    "# check whethet a matrix is positive semidefinite,\n",
    "# i.e. whether all its eigenvalues are positive\n",
    "def check_psd(A, tol=1e-8):\n",
    "    E, V = scipy.linalg.eigh(A)\n",
    "    return np.all(E > -tol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the metrics Vs. the parameter c\n",
    "def plot_metrics(results):\n",
    "\n",
    "    fontP = FontProperties()\n",
    "    fontP.set_size('small')\n",
    "\n",
    "    axes = plt.subplots(figsize=(12, 4), ncols=2, nrows=2)\n",
    "    fig = axes[0]\n",
    "    ax1 = axes[1][0][0]\n",
    "    ax2 = axes[1][0][1]\n",
    "    ax3 = axes[1][1][0]\n",
    "    ax4 = axes[1][1][1]\n",
    "\n",
    "    # accurracy plot\n",
    "    ax1.set_title('Accuracy Vs. C', fontsize=17)\n",
    "    ax1.set_ylabel('Accuracy', fontsize=13)\n",
    "    ax1.set_xlabel('C', fontsize=13)\n",
    "    line, = ax1.plot(results.C, results.Accuracy,\n",
    "             'o-', label='Accuracy on Development Set')\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # precision plot\n",
    "    ax2.set_title('Precision Vs. C', fontsize=17)\n",
    "    ax2.set_ylabel('Precision', fontsize=13)\n",
    "    ax2.set_xlabel('C', fontsize=13)\n",
    "    line, = ax2.plot(results.C, results.Precision,\n",
    "             'o-', label='Precision on Development Set')\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # recall plot\n",
    "    ax3.set_title('Recall Vs. C', fontsize=17)\n",
    "    ax3.set_ylabel('Recall', fontsize=13)\n",
    "    ax3.set_xlabel('C', fontsize=13)\n",
    "    line, = ax3.plot(results.C, results.Recall,\n",
    "             'o-', label='Recall on Development Set')\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # f1_score plot\n",
    "    ax4.set_title('F1-score Vs. C', fontsize=17)\n",
    "    ax4.set_ylabel('F1-score', fontsize=13)\n",
    "    ax4.set_xlabel('C', fontsize=13)\n",
    "    line, = ax4.plot(results.C, results.F1_score,\n",
    "             'o-', label='Accuracy on Development Set')\n",
    "    ax4.grid(True)\n",
    "    plt.suptitle('Classification metrics Vs. C', fontsize=16)\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = \"gisette_scale\"\n",
    "lines = 5500\n",
    "target, point = read_data(file, lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "Split data into train, development and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split into train and developlment set in a balanced way (i.e. same number of observations from each class)\n",
    "X_train, X_test, y_train, y_test = train_test_split(point, target, test_size=0.2, random_state=0)\n",
    "# use 2748 out of 5500 data points for the training of SVMs, 1649 as development set and 1100 as test set\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X_train, y_train, test_size=0.375, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train set: 2748 observations\n",
      "Length of development set: 1649 observations\n",
      "Length of test set: 1100 observations\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train set:\", len(X_train), \"observations\")\n",
    "print(\"Length of development set:\", len(X_dev), \"observations\")\n",
    "print(\"Length of test set:\", len(X_test), \"observations\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Mercer's condition for kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mercer's theorem is a representation of a symmetric positive-definite function on a square as a sum of a convergent sequence of product functions. if K is a valid kernel then the corresponding Kernel matrix K ∈ Rm×m must be symmetric positive semidefinite.\n",
    "This is not only a necessary, but also a sufficient condition for K to be a valid kernel.\n",
    " \n",
    "Sources: http://cs229.stanford.edu/notes/cs229-notes3.pdf,    https://people.eecs.berkeley.edu/~bartlett/courses/281b-sp08/7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric: True\n",
      "Posititve Semidefinite: True\n",
      "Mercer's condition satisfied!\n"
     ]
    }
   ],
   "source": [
    "linear_kernel = linearMatrix(X_test, X_test)\n",
    "symmetric = check_symmetric(linear_kernel)\n",
    "print(\"Symmetric:\", symmetric)\n",
    "psd = check_psd(linear_kernel)\n",
    "print(\"Posititve Semidefinite:\", check_psd(linear_kernel))\n",
    "if symmetric and psd:\n",
    "    print(\"Mercer's condition satisfied!\")\n",
    "else:\n",
    "    print(\"Mercer's condition not satisfied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gaussian Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symmetric: True\n",
      "Posititve Semidefinite: True\n",
      "Mercer's condition satisfied!\n"
     ]
    }
   ],
   "source": [
    "gaussian_kernel = gaussianMatrix(X_test, X_test, sigma=1)\n",
    "symmetric = check_symmetric(linear_kernel)\n",
    "print(\"Symmetric:\", symmetric)\n",
    "psd = check_psd(linear_kernel)\n",
    "print(\"Posititve Semidefinite:\", check_psd(linear_kernel))\n",
    "if symmetric and psd:\n",
    "    print(\"Mercer's condition satisfied!\")\n",
    "else:\n",
    "    print(\"Mercer's condition not satisfied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Linear SVM (C parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning ---\n",
      "C = 1e-05\n",
      "C = 0.0001\n",
      "C = 0.001\n",
      "C = 0.01\n",
      "C = 0.1\n",
      "C = 1\n",
      "C = 10\n",
      "C = 100\n",
      "C = 1000\n",
      "C = 10000\n",
      "C = 100000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.875682</td>\n",
       "      <td>0.844734</td>\n",
       "      <td>0.926190</td>\n",
       "      <td>0.883589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.924803</td>\n",
       "      <td>0.925178</td>\n",
       "      <td>0.927381</td>\n",
       "      <td>0.926278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.955731</td>\n",
       "      <td>0.951708</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.956779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01000</td>\n",
       "      <td>0.961795</td>\n",
       "      <td>0.966387</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.962343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.959369</td>\n",
       "      <td>0.962874</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.962401</td>\n",
       "      <td>0.969807</td>\n",
       "      <td>0.955952</td>\n",
       "      <td>0.962830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.00000</td>\n",
       "      <td>0.963614</td>\n",
       "      <td>0.967626</td>\n",
       "      <td>0.960714</td>\n",
       "      <td>0.964158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.955731</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.959524</td>\n",
       "      <td>0.956677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1000.00000</td>\n",
       "      <td>0.959369</td>\n",
       "      <td>0.962874</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10000.00000</td>\n",
       "      <td>0.959369</td>\n",
       "      <td>0.959572</td>\n",
       "      <td>0.960714</td>\n",
       "      <td>0.960143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100000.00000</td>\n",
       "      <td>0.956944</td>\n",
       "      <td>0.960479</td>\n",
       "      <td>0.954762</td>\n",
       "      <td>0.957612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               C  Accuracy    Recall  Precision  F1_score\n",
       "0        0.00001  0.875682  0.844734   0.926190  0.883589\n",
       "1        0.00010  0.924803  0.925178   0.927381  0.926278\n",
       "2        0.00100  0.955731  0.951708   0.961905  0.956779\n",
       "3        0.01000  0.961795  0.966387   0.958333  0.962343\n",
       "4        0.10000  0.959369  0.962874   0.957143  0.960000\n",
       "5        1.00000  0.962401  0.969807   0.955952  0.962830\n",
       "6       10.00000  0.963614  0.967626   0.960714  0.964158\n",
       "7      100.00000  0.955731  0.953846   0.959524  0.956677\n",
       "8     1000.00000  0.959369  0.962874   0.957143  0.960000\n",
       "9    10000.00000  0.959369  0.959572   0.960714  0.960143\n",
       "10  100000.00000  0.956944  0.960479   0.954762  0.957612"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy ( 0.963614311704 ) is achieved for C = 10\n",
      "Execution time: 2650.497817993164\n"
     ]
    }
   ],
   "source": [
    "Cs = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "start = time.time()\n",
    "best_c_linear, accuracy_linear, df_linear = tune_c(X_train, y_train, X_dev, y_dev, Cs)  \n",
    "end = time.time()\n",
    "print(\"Execution time:\", end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1AAAAEdCAYAAAACQoaqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nyuQEyszMzMzMrEz/H4i8NCB/gvdKAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b39fa72080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_metrics(df_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning SVM with Gaussian kernel (C and sigma parameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuning ---\n",
      "C: 0.0001 sigma: 0.0001\n",
      "C: 0.0001 sigma: 0.001\n",
      "C: 0.0001 sigma: 0.01\n",
      "C: 0.0001 sigma: 0.1\n",
      "C: 0.0001 sigma: 1\n",
      "C: 0.0001 sigma: 10\n",
      "C: 0.0001 sigma: 100\n",
      "C: 0.0001 sigma: 1000\n",
      "C: 0.0001 sigma: 10000\n",
      "C: 0.001 sigma: 0.0001\n",
      "C: 0.001 sigma: 0.001\n",
      "C: 0.001 sigma: 0.01\n",
      "C: 0.001 sigma: 0.1\n",
      "C: 0.001 sigma: 1\n",
      "C: 0.001 sigma: 10\n",
      "C: 0.001 sigma: 100\n",
      "C: 0.001 sigma: 1000\n",
      "C: 0.001 sigma: 10000\n",
      "C: 0.01 sigma: 0.0001\n",
      "C: 0.01 sigma: 0.001\n",
      "C: 0.01 sigma: 0.01\n",
      "C: 0.01 sigma: 0.1\n",
      "C: 0.01 sigma: 1\n",
      "C: 0.01 sigma: 10\n",
      "C: 0.01 sigma: 100\n",
      "C: 0.01 sigma: 1000\n",
      "C: 0.01 sigma: 10000\n",
      "C: 0.1 sigma: 0.0001\n",
      "C: 0.1 sigma: 0.001\n",
      "C: 0.1 sigma: 0.01\n",
      "C: 0.1 sigma: 0.1\n",
      "C: 0.1 sigma: 1\n",
      "C: 0.1 sigma: 10\n",
      "C: 0.1 sigma: 100\n",
      "C: 0.1 sigma: 1000\n",
      "C: 0.1 sigma: 10000\n",
      "C: 1 sigma: 0.0001\n",
      "C: 1 sigma: 0.001\n",
      "C: 1 sigma: 0.01\n",
      "C: 1 sigma: 0.1\n",
      "C: 1 sigma: 1\n",
      "C: 1 sigma: 10\n",
      "C: 1 sigma: 100\n",
      "C: 1 sigma: 1000\n",
      "C: 1 sigma: 10000\n",
      "C: 10 sigma: 0.0001\n",
      "C: 10 sigma: 0.001\n",
      "C: 10 sigma: 0.01\n",
      "C: 10 sigma: 0.1\n",
      "C: 10 sigma: 1\n",
      "C: 10 sigma: 10\n",
      "C: 10 sigma: 100\n",
      "C: 10 sigma: 1000\n",
      "C: 10 sigma: 10000\n",
      "C: 100 sigma: 0.0001\n",
      "C: 100 sigma: 0.001\n",
      "C: 100 sigma: 0.01\n",
      "C: 100 sigma: 0.1\n",
      "C: 100 sigma: 1\n",
      "C: 100 sigma: 10\n",
      "C: 100 sigma: 100\n",
      "C: 100 sigma: 1000\n",
      "C: 100 sigma: 10000\n",
      "C: 1000 sigma: 0.0001\n",
      "C: 1000 sigma: 0.001\n",
      "C: 1000 sigma: 0.01\n",
      "C: 1000 sigma: 0.1\n",
      "C: 1000 sigma: 1\n",
      "C: 1000 sigma: 10\n",
      "C: 1000 sigma: 100\n",
      "C: 1000 sigma: 1000\n",
      "C: 1000 sigma: 10000\n",
      "C: 10000 sigma: 0.0001\n",
      "C: 10000 sigma: 0.001\n",
      "C: 10000 sigma: 0.01\n",
      "C: 10000 sigma: 0.1\n",
      "C: 10000 sigma: 1\n",
      "C: 10000 sigma: 10\n",
      "C: 10000 sigma: 100\n",
      "C: 10000 sigma: 1000\n",
      "C: 10000 sigma: 10000\n",
      "C: 100000 sigma: 0.0001\n",
      "C: 100000 sigma: 0.001\n",
      "C: 100000 sigma: 0.01\n",
      "C: 100000 sigma: 0.1\n",
      "C: 100000 sigma: 1\n",
      "C: 100000 sigma: 10\n",
      "C: 100000 sigma: 100\n",
      "C: 100000 sigma: 1000\n",
      "C: 100000 sigma: 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C</th>\n",
       "      <th>Sigma</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0001</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0010</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.491207</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.002378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>100.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>100.0000</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.775015</td>\n",
       "      <td>0.919499</td>\n",
       "      <td>0.611905</td>\n",
       "      <td>0.734811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>100.0000</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.963614</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.964286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.775015</td>\n",
       "      <td>0.919499</td>\n",
       "      <td>0.611905</td>\n",
       "      <td>0.734811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1000.0000</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.964221</td>\n",
       "      <td>0.968788</td>\n",
       "      <td>0.960714</td>\n",
       "      <td>0.964734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.775015</td>\n",
       "      <td>0.916519</td>\n",
       "      <td>0.614286</td>\n",
       "      <td>0.735567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.961189</td>\n",
       "      <td>0.966346</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.961722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>10.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>100.0000</td>\n",
       "      <td>0.490600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>1000.0000</td>\n",
       "      <td>0.775015</td>\n",
       "      <td>0.919499</td>\n",
       "      <td>0.611905</td>\n",
       "      <td>0.734811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>100000.0000</td>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.965434</td>\n",
       "      <td>0.969988</td>\n",
       "      <td>0.961905</td>\n",
       "      <td>0.965929</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              C       Sigma  Accuracy    Recall  Precision  F1_score\n",
       "0        0.0001      0.0001  0.490600  0.000000   0.000000  0.000000\n",
       "1        0.0001      0.0010  0.490600  0.000000   0.000000  0.000000\n",
       "2        0.0001      0.0100  0.490600  0.000000   0.000000  0.000000\n",
       "3        0.0001      0.1000  0.490600  0.000000   0.000000  0.000000\n",
       "4        0.0001      1.0000  0.490600  0.000000   0.000000  0.000000\n",
       "5        0.0001     10.0000  0.490600  0.000000   0.000000  0.000000\n",
       "6        0.0001    100.0000  0.490600  0.000000   0.000000  0.000000\n",
       "7        0.0001   1000.0000  0.490600  0.000000   0.000000  0.000000\n",
       "8        0.0001  10000.0000  0.490600  0.000000   0.000000  0.000000\n",
       "9        0.0010      0.0001  0.490600  0.000000   0.000000  0.000000\n",
       "10       0.0010      0.0010  0.490600  0.000000   0.000000  0.000000\n",
       "11       0.0010      0.0100  0.490600  0.000000   0.000000  0.000000\n",
       "12       0.0010      0.1000  0.490600  0.000000   0.000000  0.000000\n",
       "13       0.0010      1.0000  0.490600  0.000000   0.000000  0.000000\n",
       "14       0.0010     10.0000  0.490600  0.000000   0.000000  0.000000\n",
       "15       0.0010    100.0000  0.490600  0.000000   0.000000  0.000000\n",
       "16       0.0010   1000.0000  0.490600  0.000000   0.000000  0.000000\n",
       "17       0.0010  10000.0000  0.490600  0.000000   0.000000  0.000000\n",
       "18       0.0100      0.0001  0.490600  0.000000   0.000000  0.000000\n",
       "19       0.0100      0.0010  0.490600  0.000000   0.000000  0.000000\n",
       "20       0.0100      0.0100  0.490600  0.000000   0.000000  0.000000\n",
       "21       0.0100      0.1000  0.490600  0.000000   0.000000  0.000000\n",
       "22       0.0100      1.0000  0.490600  0.000000   0.000000  0.000000\n",
       "23       0.0100     10.0000  0.490600  0.000000   0.000000  0.000000\n",
       "24       0.0100    100.0000  0.490600  0.000000   0.000000  0.000000\n",
       "25       0.0100   1000.0000  0.490600  0.000000   0.000000  0.000000\n",
       "26       0.0100  10000.0000  0.491207  1.000000   0.001190  0.002378\n",
       "27       0.1000      0.0001  0.490600  0.000000   0.000000  0.000000\n",
       "28       0.1000      0.0010  0.490600  0.000000   0.000000  0.000000\n",
       "29       0.1000      0.0100  0.490600  0.000000   0.000000  0.000000\n",
       "..          ...         ...       ...       ...        ...       ...\n",
       "60     100.0000    100.0000  0.490600  0.000000   0.000000  0.000000\n",
       "61     100.0000   1000.0000  0.775015  0.919499   0.611905  0.734811\n",
       "62     100.0000  10000.0000  0.963614  0.964286   0.964286  0.964286\n",
       "63    1000.0000      0.0001  0.490600  0.000000   0.000000  0.000000\n",
       "64    1000.0000      0.0010  0.490600  0.000000   0.000000  0.000000\n",
       "65    1000.0000      0.0100  0.490600  0.000000   0.000000  0.000000\n",
       "66    1000.0000      0.1000  0.490600  0.000000   0.000000  0.000000\n",
       "67    1000.0000      1.0000  0.490600  0.000000   0.000000  0.000000\n",
       "68    1000.0000     10.0000  0.490600  0.000000   0.000000  0.000000\n",
       "69    1000.0000    100.0000  0.490600  0.000000   0.000000  0.000000\n",
       "70    1000.0000   1000.0000  0.775015  0.919499   0.611905  0.734811\n",
       "71    1000.0000  10000.0000  0.964221  0.968788   0.960714  0.964734\n",
       "72   10000.0000      0.0001  0.490600  0.000000   0.000000  0.000000\n",
       "73   10000.0000      0.0010  0.490600  0.000000   0.000000  0.000000\n",
       "74   10000.0000      0.0100  0.490600  0.000000   0.000000  0.000000\n",
       "75   10000.0000      0.1000  0.490600  0.000000   0.000000  0.000000\n",
       "76   10000.0000      1.0000  0.490600  0.000000   0.000000  0.000000\n",
       "77   10000.0000     10.0000  0.490600  0.000000   0.000000  0.000000\n",
       "78   10000.0000    100.0000  0.490600  0.000000   0.000000  0.000000\n",
       "79   10000.0000   1000.0000  0.775015  0.916519   0.614286  0.735567\n",
       "80   10000.0000  10000.0000  0.961189  0.966346   0.957143  0.961722\n",
       "81  100000.0000      0.0001  0.490600  0.000000   0.000000  0.000000\n",
       "82  100000.0000      0.0010  0.490600  0.000000   0.000000  0.000000\n",
       "83  100000.0000      0.0100  0.490600  0.000000   0.000000  0.000000\n",
       "84  100000.0000      0.1000  0.490600  0.000000   0.000000  0.000000\n",
       "85  100000.0000      1.0000  0.490600  0.000000   0.000000  0.000000\n",
       "86  100000.0000     10.0000  0.490600  0.000000   0.000000  0.000000\n",
       "87  100000.0000    100.0000  0.490600  0.000000   0.000000  0.000000\n",
       "88  100000.0000   1000.0000  0.775015  0.919499   0.611905  0.734811\n",
       "89  100000.0000  10000.0000  0.965434  0.969988   0.961905  0.965929\n",
       "\n",
       "[90 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy ( 0.965433596119 ) is achieved for C = 100000  and sigma = 10000\n",
      "Execution time: 10478.164714574814\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000]\n",
    "sigmas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "start = time.time()\n",
    "best_c_gaussian, best_sigma, accuracy_gaussian, gaussian_df = tune_gaussian(X_train, y_train, X_dev, y_dev, Cs, sigmas)  \n",
    "end = time.time()\n",
    "print(\"Execution time:\", end-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution with best parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.498181818182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49818181818181817"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline(y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Linear SVM on test set: 0.960909090909\n"
     ]
    }
   ],
   "source": [
    "alpha, beta, w = smo(y_train, X_train, best_c_linear)\n",
    "us = eval_svm(alpha, beta, X_train, X_test, y_train, \"linear\")\n",
    "accuracy_linear = accuracy(us, y_test)\n",
    "print(\"Accuracy Linear SVM on test set:\", accuracy_linear)\n",
    "\n",
    "alpha_linear = pd.DataFrame(alpha, columns=[\"Alphas\"])\n",
    "alpha_linear.to_csv(\"alpha_linear.csv\")\n",
    "\n",
    "beta_linear = pd.DataFrame([beta], columns=[\"Beta\"])\n",
    "beta_linear.to_csv(\"beta_linear.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM with Gaussian kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy SVM with Gaussian kernel on test set: 0.967272727273\n"
     ]
    }
   ],
   "source": [
    "alpha, beta, w = smo(y_train, X_train, best_c_gaussian, \"gaussian\", best_sigma)\n",
    "us = eval_svm(alpha, beta, X_train, X_test, y_train, \"gaussian\", best_sigma)\n",
    "accuracy_gaussian = accuracy(us, y_test)\n",
    "\n",
    "print(\"Accuracy SVM with Gaussian kernel on test set:\", accuracy_gaussian)\n",
    "\n",
    "alpha_gaussian = pd.DataFrame(alpha, columns=[\"Alphas\"])\n",
    "alpha_gaussian.to_csv(\"alpha_gaussian.csv\")\n",
    "\n",
    "beta_gaussian = pd.DataFrame([beta], columns=[\"Beta\"])\n",
    "beta_gaussian.to_csv(\"beta_gaussian.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
